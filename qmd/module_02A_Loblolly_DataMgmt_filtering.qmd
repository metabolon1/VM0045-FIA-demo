---
title: "MODULE 2A - Loblolly pine forests: data management"
date: 2025-11-08
author: 
  - name: Taylan Morcol
    google_scholar: https://scholar.google.com/citations?user=OAvldLMAAAAJ&hl=en
    website: https://morcol.com 
    linkedin: https://www.linkedin.com/in/taylan-morcol
format: html
output-file: module_02A_Loblolly_DataMgmt_filtering.html
---




MODULE UNDER CONSTRUCTION...





[‚Üê Back to module index](index.html)

**Disclaimer:** This material was prepared on my own personal time and is offered in my personal capacity only. It does not represent the views of, nor is it endorsed by, any current or past employer.

# ---SUMMARY---
In the last module, working with a small set of FIA plots from Rhode Island, we saw how to apply some of the VM0045 donor pool selection criteria. In this module, we turn our attention to the Southeastern US. Targetting a larger geographic area this time, we apply the full set of donor selection criteria to FIA plots in loblolly pine forests in ecological section 232J (i.e., Southern Atlantic Coastal Plains and Flatwoods section of the Outer Coastal Plain Mixed Forest province). 

![Ecological section 232J (pink outline) interects four US states (northeast to southwest): North Carolina, South Carolina (blue outline), Georgia (blue outline), and Florida. Map excerpt from Cleland et al., (2007), https://doi.org/10.2737/WO-GTR-76D. Outlines added for clarity.](/images/Cleland_etAl_2007_section232J.png)

[delete?] We then select an FIA plot to serve as a _pseudo_-sample unit and demonstrate the composite baseline matching procedure.



As seen in the map, section 232J intersects 4 states, each of which is substantially larger than Rhode Island. Thus, this module deals with much larger data files than the first module. As we'll see, this requires some modifications to how we download, read in, and process the FIA data. These modifications go beyond what is presented in the [rFIA big data guide](https://doserlab.com/files/rfia/articles/bigdata), which is only concerned with unit-level estimation, because this module deals with plot-level estimation.

In a future module, I'll demonstrate the matching procedure for multiple sample units, constructing a composite baseline for each one. With multiple samples, one can then assess match quality, another necessary aspect of VM0045. Thus, this module and the next approximate how one might approach generating a composite baseline for an IFM project involving loblolly pine plantings.

# ---R PIPELINE---
## Load packages
In addition to the packages used in Module 01, this module also uses:

- `here`, which is used to find current working directory

- `parallel`, which is used to determine the number of cores available for parallel processing.

- `callr`, which allows us to run isolated, one-off sessions of R during data loading and carbon calculations. This helps with memory management issues that come with dealing with large data files and RAM intensive processes.

- `geosphere`, which is used for calculating geographic distance, one of the covariates involved in the matching procedure.

- `optmatch`, which is used for calculating Mahalanobis distances for the baseline matching procedure.

```{r, message=FALSE}
# packages from Module 01
library(rFIA)
library(dplyr)
library(tidyr)
library(ggplot2)
library(maps)

# packages new to this module
library(parallel)
library(callr)
library(optmatch)
library(geosphere)
library(here)
```

## Download data from FIADB  #req_map$req_both <- req_map$req_carbon & req_map$req_print

This is where things start to get tricky. In Module 1, I purposefully selected a small geographic area (Rhode Island) to keep things simple and avoid having to deal with memory management. But in this module, we're dealing with a larger dataset. My personal laptop running Ubuntu 20.04 has 12 GB RAM, but only ~9 GB is available after allocating to default, background processes. So, to get the script to run without crashing, I had to employ some different tactics.

The first difference is that the combined FIADB files download for North Carolina, South Carolina, Georgia, and Florida is a much larger size than Rhode Island, so much so that loading the data directly into RAM via `rFIA::getFIA()` becomes impractical on a personal computer. 

I employed two strategies to deal with this:

1. Specify `getFIA()` to only download the tables required by `rFIA::carbon()`, a function used later to calculate carbon/acre. Info on required tables is found in the [function definition for `rFIA::carbonStarter`](https://github.com/doserjef/rFIA/blob/master/R/carbonStarter.R), an internal function that is called by `carbon()`. 

2. Specify `getFIA()` to download files to disk rather than loading into RAM.

```{r}
#| label: setup_download_params
#| echo: true
#| eval: true

# 2-letter code for each desired US state
states  <- c("NC","SC","GA","FL")

# minimal tables required for carbon calcs
carb_tbls <- c("PLOT", "TREE", "COND", "POP_PLOT_STRATUM_ASSGN", 
               "POP_ESTN_UNIT", "POP_EVAL", "POP_STRATUM", 
               "POP_EVAL_TYP", "POP_EVAL_GRP", "PLOTGEOM")

# file path to directory where raw FIADB csv files will be downloaded
dl_dir <- paste0(here(), "/data/raw/")
```

```{r}
#| label: download_FIA_data
#| echo: true
#| eval: false

# set timeout to 1 hour, otherwise R will abort longer downloads
options(timeout=3600)
 
# get time just before downloads start (for benchmarking)
t0 <- Sys.time()

# download FIADB files
FIADB <- getFIA(states = states, # specify which states' data to download
                tables = carb_tbls, # specifiy which tables to download
                load = FALSE,       # don't load data into RAM...
                dir = dl_dir)       # ... instead, download to specified directory

# get time after downloads finish (for benchmarking)
t1 <- Sys.time() 

# calculate the time it took to run the downloads
t1 - t0
```

The downloaded csv files total about 2.8 GB, and the download took about 5.3 minutes on my laptop. Right afterwards, I ran a speed test at [speedtest.net](https://www.speedtest.net). My download speed during the test was about 10x faster than while downloading the FIADB files, suggesting that download speeds were limited by the FIA server.

## Determine necessary columns
The next thing I did to ease the burden on computing resources was select only necessary columns when importing the tables into R. FIADB tables often have tens to hundreds of columns, but `rFIA:carbon()` only needs a small handful of those. By reducing the number of columns, we can substantially reduce the size of the imported data. 

My first step was to point ChatGPT to the [rFIA source code repo](https://github.com/doserjef/rFIA/tree/master/R) and ask it determine which columns in each of the 10 FIADB table are requried by `carbon()`. Then, I imported those tables, selected only the indicated columns, and ran `carbon()`. It failed, saying it couldn't find a necessary column. 

I then asked ChatGPT to double check it's list of columns against the repo and try again. Again it returned a list of columns, and again `carbon()` failed. 

Taking a closer look at the rFIA code, I saw that required columns are not always stated explicitly. This makes it a difficult task to determine which columns are actually needed. I needed a more systematic approach.

I had noticed that each time `carbon()` failed, the error message named a specific column it was looking for that it couldn't find. So I got the idea to run a series of leave-one-out experiments: for each column in each table, leave that column out, and run `carbon()`. Whenever `carbon()` fails to run, note which column was left out. Those columns are the ones that `carbon()` cannot do without. 

While I could write all the code myself, it would take a while. So I described the test I wanted to run, and had ChatGPT write some code for me. The first script it gave me was too complicated and hard to follow, so I asked it for minimally viable code. After a couple back-and-forths, I arrived at the code below, with some modifications. I'll walk through it step-by-step.

### Leave-one-out-test: Step 1 - build a tiny test dataset
The `carbon()` function can be fairly time consuming to run, depending on how much data you throw at it. Since my leave-one-out experiment calls `carbon()` hundreds of times (once for each column in each table), I wanted to give it the minimal amount of data needed to run the experiment. I reasoned that tables with one row each should be enough. So, using the Florida csv files downloaded earlier and filtering to retain only the first row of each table, I constructed a tiny data object for testing.

```{r}
## Minimal leave-one-out test for rFIA::carbon(byPlot = TRUE)

## load tables into memory
temp <- lapply(carb_tbls, 
               function(tbl){readFIA(dir = dl_dir, # path to csv files
                                     tables = tbl, # table name
                                     states = "FL", # state
                                     nrows = 1 # first row from each table
                                    )[[tbl]]}) # extract dataframe from list

names(temp) <- carb_tbls # add table names back in
FIADB_test <- c(temp) # combine the output objects
class(FIADB_test) <- "FIA.Database" # re-assign correct class
```

The last line of code assigns the class `FIA.Database` to the list in which the tables are stored. When operating on data stored in memory (as opposed to stored on disk) `carbon()` will only accept a data object of this class. While coercing a list to class `FIA.Database` works fine for the current application, it's possible this breaks some functionality is other aspects of `rFIA`.

### Leave-one-out test: Step 2 - verify tiny test dataset
Before running the experiment, it's important to check if `carbon()` actually will accept tables with just one row each.

```{r}
try(carbon(FIADB_test, byPlot = TRUE, nCores = 1))
```

The code runs without error, so we're good to go on to the next step.

### Leave-one-out test: Step 3 - run experiment
The code below removes each column one at a time. For each missing column, it does carbon calculations (single core, per-plot basis) and tries printing the object. If either `carbon()` or `print()` throw an error, it records the name of the corresponding missing column in a list. For each table, it also keeps track of the names and numbers of columns required by each function. 

I included printing in the error checking, because printing an `FIA.Database` object requires certain table columns. I have found that manipulating `FIA.Database` objects using functions outside of `rFIA` (e.g., using `dplyr::select()`) can break certain functionalities. Printing the database is one of these. Some of these issues I've decided are not critical, so I ignore them. Others, like losing the ability to easily print the database to the terminal, are harder to live without. So in the case of printing, I decided it was worthwhile to figure out which columns from which tables need to be retained.

On my laptop, the code below takes ~4.5 minutes to run.

```{r}
## ---- leave-one-column-out per table -----------------------------------

t0 <- Sys.time() # record time at beginning

# initialize list to hold names of requried columns
req_cols_info <- setNames(vector("list", length(carb_tbls)), carb_tbls)

# initialize list to hold summary stats
summary_rows <- list()

for (tbl in carb_tbls) { # loop through each table name
  dt <- FIADB_test[[tbl]] # extract single table/dataframe
  cols <- names(dt) # extract column names
  
  # initialize holder for names of columns requried for carbon() & print()
  req_carbon <- character(0)
  req_print  <- character(0)

  for (col in cols) {
    db2 <- FIADB_test # copy full tiny db
    db2[[tbl]] <- dt %>% select(-all_of(col)) # replace table with dropped version
    
    # try running carbon() & record any error m  #req_map$req_both <- req_map$req_carbon & req_map$req_print
    tc <- try(carbon(db2, byPlot = TRUE, nCores = 1), silent = TRUE)

    # try printing db2 & record any error msgs
    tp <- try(capture.output(print(db2)), silent = TRUE)

    if(inherits(tc, "try-error")){req_carbon <- c(req_carbon, col)} # mark column as required on any error
    if(inherits(tp, "try-error")){req_print <- c(req_print, col)} # mark column as required on any error
  }

  # per-column flag map for this table
  req_map <- data.frame(
    col        = cols,
    req_carbon = cols %in% req_carbon,
    req_print  = cols %in% req_print,
    stringsAsFactors = FALSE
  )

  # add in logical columns for if the FIA column is required by both or either
  req_map$req_any  <- req_map$req_carbon | req_map$req_print

  req_cols_info[[tbl]] <- req_map # update the holder list

  total <- length(cols) # total number of cols in table
  n_carbon <- sum(req_map$req_carbon) # number of cols req'd by carbon()
  n_any    <- sum(req_map$req_any)    # number of cols req'd by either


  summary_rows[[tbl]] <- data.frame(
    table         = tbl,
    req_carbon_n  = n_carbon,
    req_any_n     = n_any,
    tot_cols      = total,
    req_any_frac  = sprintf("%d/%d", n_any, total),
    req_any_pct   = round(100 * n_any/total, 1),
    stringsAsFactors = FALSE)
}

t1 <- Sys.time() # record time at end

t1 - t0 # time to run code chunk
```


### Leave-one-out test: Step 4 - examine results
Let's look at the column totals from the leave-one-out test.
```{r}
## ---- results -----------------------------------------------------------
summary_df <- do.call(rbind, summary_rows) # rbind all rows

# totals across tables
tot_carbon <- sum(summary_df$req_carbon_n)
tot_any    <- sum(summary_df$req_any_n)
tot_cols   <- sum(summary_df$tot_cols)

tot_row <- data.frame(
  table              = "totals",
  req_carbon_n  = tot_carbon,
  req_any_n     = tot_any,
  tot_cols         = tot_cols,
  req_any_frac  = sprintf("%d/%d", tot_any, tot_cols),
  req_any_pct   = if (tot_cols) round(100 * tot_any / tot_cols, 1) else NA_real_,
  stringsAsFactors = FALSE
)

summary_df <- rbind(summary_df, tot_row)

print(summary_df, row.names = FALSE, width = 500)
```

Some notable observations:

- Across the 10 tables, only 62 out 504 columns (11.9%) are required for both `carbon()` and `print()` to function properly. 

- At most 10 columns are required from any one table.

- No columns are required from the POP_EVAL_GRP table, even though it is listed as a required table in `rFIA` source code. I am not sure why this is. But `carbon()` throws an error if this table is missing altogether.

- TREE and COND tables have the greatest numbers of total columns. 

- Only ~5% of columns in the TREE table are requried. This represents a substantial reduction in overall database size: among the four states in this module (i.e., FL, GA, NC, SC), the TREE table is 4-7x larger in file size than the other nine tables _combined_. 

- Comparing `req_carbon_n` vs. `req_any_n`, we see that only two additional columns are needed to also make printing functional: one from POP_EVAL_GRP and one from POP_STRATUM. Given how relatively small these two tables are, the effect of adding these two additional columns on database size is negligible. 

We can also view the names of the required columns:

```{r}
# extract required column names
req_cols_list <- lapply(req_cols_info, function(x){x$col[x$req_any]})

req_cols_list # print required column names
```

Note that each non-empty table contains at least one "CN" type of column. These are unique record identifiers and are likely also used for joins within `carbon()`.

### Leave-one-out test: Step 5 - validate results
The last step is to construct a subset of the tiny test set that contains only the putatively required tables and columns and see if `carbon()` still runs without error.

```{r}
## ---- test if minimal set of columns still works -------------------------------------

# select only required columns for each table
temp <- lapply(names(req_cols_list), 
               function(tbl){FIADB_test[[tbl]] %>% select(all_of(req_cols_list[[tbl]]))})

names(temp) <- carb_tbls # add table names back in
FIADB_req <- c(temp) # combine the output objects
class(FIADB_req) <- "FIA.Database" # re-assign correct class

# confirm for each table, compare number of columns: full vs. required
cbind(full = sapply(FIADB_test, ncol), req = sapply(FIADB_req, ncol))

# test if carbon() still works on reduced tables
cat("Testing carbon() on reduced FIADB...\n")
res <- try(rFIA::carbon(FIADB_req, byPlot = TRUE, nCores = 1), silent = TRUE)
if (inherits(res, "try-error")) {
  cat("carbon() failed:\n", conditionMessage(attr(res, "condition")), "\n")
} else {cat("carbon() succeeded.\n")}

# test if print() still works on reduced tables
cat("Testing print() on reduced FIADB...\n")
res <- try(capture.output(print(FIADB_req)), silent = TRUE)
if (inherits(res, "try-error")) {
  cat("print() failed:\n", conditionMessage(attr(res, "condition")), "\n")
} else {cat("print() succeeded.\n")}
```

The number of columns in the reduced table is what we expect, and both `carbon()` and `print()` run without error. This means we have apparently identified the minimal set of columns required to move forward.

## Include additional variables needed for VM0045
Beyond the 62 columns we've selected, we need a few additional columns for VM0045 donor pool selection and baseline matching. Thankfully, all of these additional columns are found with the 10 FIADB tables we've already downloaded.

```{r}
# copy column names to new object
req_cols_plus_list <- req_cols_list

# add additional column names for VM0045
req_cols_plus_list$PLOT <- c(req_cols_list$PLOT, "PREV_PLT_CN", "CYCLE", 
                        "MEASMON", "MEASDAY", "KINDCD", "RDDISTCD", 
                        "ELEV")

req_cols_plus_list$COND <- c(req_cols_list$COND, "STDORGCD", "FORTYPCD",
                        "OWNGRPCD", "STDAGE", "SITECLCD", "SLOPE")

req_cols_plus_list$TREE <- c(req_cols_list$TREE, "SPGRPCD", "TREECLCD")

req_cols_plus_list$PLOTGEOM <- c(req_cols_list$PLOTGEOM, "ECOSUBCD")

req_cols_plus_list$POP_EVAL_GRP <- "CN" # to prevent errors downstream
```

## Load FIADB tables into R
At this stage, we use the columns we've identified to load tables into R using `readFIA`. I use `callr:r` to do the loading process in an isolated session of R, which returns unused RAM back to the OS before terminating. The process requires about 1.4 GB RAM, whereas the final object returned is 0.46 GB, so it helps to return the unused RAM back to the OS.

```{r}
## load tables with only required columns into memory
FIADB <- callr::r(function(req_cols_plus_list, req_cols_info, dl_dir, states){
   library(rFIA) # load rFIA in the new/isolated R session
   FIADB <- list() # initialize an empty list

   for(tbl in names(req_cols_plus_list)){ # loop over each table
      all_cols <- req_cols_info[[tbl]]$col # names of all columns in table
      req_cols <- req_cols_plus_list[[tbl]] # names of requried columns in table
      drop_cols <- all_cols[!all_cols %in% req_cols] # names of columns to drop from table

      FIADB[[tbl]] <- readFIA(dir = dl_dir, # path to csv files
                             tables = tbl, # table name
                             states = states, # state
                             colClasses = list(`NULL` = drop_cols)
                             )[[tbl]]} # end for() loop

   class(FIADB) <- "FIA.Database" # re-assign correct class
   FIADB # return the object
   }, # end function() for callr::r

   # list of arguments/objects to pass from main session to isolated session
   args = list(req_cols_plus_list = req_cols_plus_list, 
               req_cols_info = req_cols_info,
               dl_dir = dl_dir,
               states = states)
) # end callr::r

print(object.size(FIADB), units = "GB", digits = 2) # size of output disk
```


## Filter by PLT_CN
As we saw in the [Module 1](https://taylanmorcol.github.io/VM0045-FIA-demo/module_01_filtering_demo_RI.html#exploring-effects-of-filters-on-distribution-of-carbonacre-data), filtering by plot visits can substantially reduce the size of the dataset. In that module, we were dealing with a relatively small dataset, so we ran carbon calculations before filtering by plot visit. But since we're dealing with a much larger dataset this time, it would be wise to filter the dataset before running `carbon()`.

In Module 1, I only applied what I call the _absolute_ filters, because I hadn't selected characteristics for a sample unit. But in this module, I can also apply the three _relative_ filtering criteria, because I have the following sample unit characteristics in mind:

- Loblolly pine forest `FORTYPCD == 161`

- Ecological section 232J (Southern Atlantic Coastal Plains and Flatwoods)

- Artificial regeneration - `STDORGCD == 1`

### Determine latest cycle for each state
Before filtering, we need to determine the latest completed cycle, which is one of the _absolute_ selection criteria. As we did in Module 1, we inspect a contingency table of plot vists by cycle number but also by state code, since states can be on different cycles. We use the same reasoning as in Module 1 to select the second-to-last cycle number with completed visits for each state.

```{r}
# use to determine latest COMPLETED cycle by state (for one of filters below)
table(CYCLE = FIADB$PLOT$CYCLE, STATECD = FIADB$PLOT$STATECD)
```

Based on the table above, the latest completed cycle by state code is (`STATECD`/`CYCLE`): 12/10, 13/11, 37/10, 45/12. For the current purpose, it's not critical to know which state code corresponds to which state.

### Determine plot visits that meet selection criteria
We are now ready for the next step: determining the `PLT_CN`'s (i.e., plot visits) that meet our selection criteria. We will then use those `PLT_CN`'s to filter the database down to only the visits we want. 

The filtering variables are found across three tables--PLOT, COND, and PLOTGEOM--so we begin by joining those. We then derive two new filtering variables: `meas_date` and `eco_section`. We apply all the filters--both _relative_ and _absolute_--and the select just the `PLT_CN`'s to carry forward to the next step.
```{r}
# filter down to necessary list of CNsSouthern Atlantic Coastal Plains and Flatwoods
PLT_CN_keep <- FIADB$PLOT %>%
               inner_join(FIADB$COND, 
                          by = c("CN" = "PLT_CN"), 
                          suffix = c("",".y")) %>%
               inner_join(FIADB$PLOTGEOM, 
                          by = "CN", 
                          suffix=c("",".y")) %>%
               select(-ends_with(".y")) %>%
               mutate(meas_date = as.Date(paste(MEASYEAR, MEASMON, MEASDAY, sep="-"),
                                          format = "%Y-%m-%d"), 
                      eco_section = sub("[a-z]$", "", ECOSUBCD)) %>%               
               filter(!CN %in% unique(PREV_PLT_CN) &
              length(PLT_CN_keep)
        KINDCD == 2 &
                      ((STATECD %in% c(12, 37) & CYCLE >= 10) |
                       (STATECD == 13 & CYCLE >= 11) |
                       (STATECD == 45 & CYCLE >= 12)) &
                      CONDPROP_UNADJ == 1 &
                      eco_section == "232J" &
                      STDORGCD == 1 &
                      FORTYPCD == 161 &
                      as.Date ("2025-11-01") - meas_date <= 10 * 365.25) %>%
               select(CN) %>%
               unlist() %>%
               unique()
names(PLT_CN_keep) <- NULL
```

As we can see below, these filters reduced the number of plot visits from 
```{r}
length(PLT_CN_keep)

```

### Filter by PLT_CN
There are five tables in our database that contain plot sequence numbers, either `CN` or `PLT_CN` depending on the table. We apply the `PLT_CN` selection to each of these five tables.

```{r}
# copy the database
FIADB_f <- FIADB

# filter the copied database for only PLT_CN's we want to keep (not all tables have this)
FIADB_f$PLOT <- FIADB$PLOT %>% filter(CN %in% PLT_CN_keep)
FIADB_f$PLOTGEOM <- FIADB$PLOTGEOM %>% filter(CN %in% PLT_CN_keep)
FIADB_f$TREE <- FIADB$TREE %>% filter(PLT_CN %in% PLT_CN_keep)
FIADB_f$COND <- FIADB$COND %>% filter(PLT_CN %in% PLT_CN_keep)
FIADB_f$POP_PLOT_STRATUM_ASSGN <- FIADB$POP_PLOT_STRATUM_ASSGN %>% filter(PLT_CN %in% PLT_CN_keep)
```

Let's compare the two databases in terms of size on disk and rows of data.
```{r}
# compare size on disk
F_sz <- object.size(FIADB)
Ff_sz <- object.size(FIADB_f)
print(F_sz, units = "MB")
print(Ff_sz, units = "MB")
cat("Filtereing reduced file size by", round(100 * (1- Ff_sz/F_sz), 2), "%\n")

# compare number of rows for each table
F_row <- sapply(FIADB, nrow) 
Ff_row <- sapply(FIADB_f, nrow)
pct_red <- round(100 *  (1 - Ff_row/F_row), 1)
cbind(FIADB_rows = F_row, 
      FIADB_f_rows = Ff_row,
      pct_reduction = pct_f) %>% 
  data.frame() %>%
  arrange(desc(FIADB_rows))
```

We see that filtering the database reduced size on disk by 99.6% (from 471 MB to 1.7 MB). And the row counts of the five largest tables by original row count were each reduced by at least 99.8% each. Note that because the other five tables don't contain plot-visit-level data, they were not filtered.

## Carbon calculations

```{r}
cc <- carbon(db = FIADB_f,     # object in which raw FIADB data are stored
             byPlot = TRUE,  # perform calculations at the per-plot level
             byPool = FALSE) # do NOT split carbon calcs by IPCC forest carbon pool (default is TRUE)

str(cc) #examine structure

# are all PLT_CN's in the carbon table also in the keep list?
all(cc$PLT_CN %in% PLT_CN_keep)
```


```{r}
print(object.size(FIADB_f), units = "MB")
print(object.size(cc), units = "KB")
```

```{r}
saveRDS(FIADB_f, file = paste0(here(), "/data/Mod02A_FIADB_f.rds"))
saveRDS(cc, file = paste0(here(), "/data/Mod02A_cc.rds"))
```


# ---SUMMARY---

